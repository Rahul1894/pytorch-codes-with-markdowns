{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as datasets \n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "#torch.nn: This module provides the building blocks to create neural networks.\n",
    "#torch.nn.functional: This module contains functions for various operations such as activations (ReLU, softmax, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #                        Tensor Indexing                        #\n",
    "# # ============================================================= #\n",
    "\n",
    "# batch_size = 10\n",
    "# features = 25\n",
    "# x = torch.rand((batch_size, features))\n",
    "\n",
    "# # Get first examples features\n",
    "# print(x[0].shape)  # shape [25], this is same as doing x[0,:]\n",
    "\n",
    "# # Get the first feature for all examples\n",
    "# print(x[:, 0].shape)  # shape [10]\n",
    "\n",
    "# # For example: Want to access third example in the batch and the first ten features\n",
    "# print(x[2, 0:10].shape)  # shape: [10]\n",
    "\n",
    "# # For example we can use this to, assign certain elements\n",
    "# x[0, 0] = 100\n",
    "\n",
    "# # Fancy Indexing\n",
    "# x = torch.arange(10)\n",
    "# indices = [2, 5, 8]\n",
    "# print(x[indices])  # x[indices] = [2, 5, 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create fully connected network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self,input_size,num_classes):\n",
    "        super(NN,self).__init__()\n",
    "        self.fc1=nn.Linear(input_size,50)  #input feature layer is of input size and then next hidden layer is of 50 nodes \n",
    "        self.fc2=nn.Linear(50,num_classes)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=F.relu(self.fc1(x))\n",
    "        x=self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "model=NN(784,10)\n",
    "x=torch.randn(64,784)\n",
    "print(model(x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down the process of how the first fully connected (linear) layer `fc1` transforms the input tensor from shape `(64, 784)` to shape `(64, 50)`.\n",
    "\n",
    "### Linear Layer Transformation\n",
    "\n",
    "The linear layer `nn.Linear(input_size, 50)` performs a linear transformation on the input data. Here, `input_size` is 784, and the output size is 50. This layer can be described by the following equation:\n",
    "\n",
    "\\[ \\text{output} = xW^T + b \\]\n",
    "\n",
    "- \\( x \\): Input tensor of shape `(64, 784)` (64 samples, each with 784 features).\n",
    "- \\( W \\): Weight matrix of shape `(784,50)` (50 output features, each with 784 input features).\n",
    "- \\( b \\): Bias vector of shape `(50)` (one bias term for each of the 50 output features).\n",
    "\n",
    "### Matrix Multiplication and Bias Addition\n",
    "\n",
    "1. **Matrix Multiplication**:\n",
    "    - The input tensor `x` of shape `(64, 784)` is multiplied by the transpose of the weight matrix `W^T` of shape `(784, 50)`.\n",
    "    - The resulting product has shape `(64, 50)` because:\n",
    "      - The multiplication of a `(64, 784)` matrix with a `(784, 50)` matrix results in a `(64, 50)` matrix.\n",
    "      - Each of the 64 samples is transformed from 784 features to 50 features.\n",
    "\n",
    "2. **Bias Addition**:\n",
    "    - The bias vector `b` of shape `(50)` is added to each of the 64 rows of the resulting matrix.\n",
    "    - The addition of the bias does not change the shape, so the final output remains `(64, 50)`.\n",
    "\n",
    "### Applying ReLU Activation\n",
    "\n",
    "After the linear transformation, the ReLU activation function is applied element-wise:\n",
    "\n",
    "```python\n",
    "x = F.relu(self.fc1(x))\n",
    "```\n",
    "\n",
    "- This does not change the shape of the tensor; it remains `(64, 50)`.\n",
    "- ReLU sets all negative values in the tensor to zero, keeping positive values unchanged.\n",
    "\n",
    "### Summary\n",
    "\n",
    "- Input to `fc1`: Shape `(64, 784)`.\n",
    "- Weight matrix `W` in `fc1`: Shape `(50, 784)`.\n",
    "- Bias vector `b` in `fc1`: Shape `(50)`.\n",
    "- Output from `fc1` after linear transformation: Shape `(64, 50)`.\n",
    "- Output from `fc1` after ReLU activation: Shape `(64, 50)`.\n",
    "\n",
    "Thus, the first fully connected layer `fc1` correctly transforms the input tensor from shape `(64, 784)` to shape `(64, 50)`. This is how the output shape of `(64, 50)` is obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self,in_channels=1,num_classes=10):\n",
    "        super(CNN,self).__init__()\n",
    "        self.conv1=nn.Conv2d(in_channels=in_channels,out_channels=8,kernel_size=(3,3),stride=(1,1),padding=(1,1))\n",
    "        self.pool=nn.MaxPool2d(kernel_size=(2,2),stride=(2,2))\n",
    "        self.conv2=nn.Conv2d(in_channels=8,out_channels=16,kernel_size=(3,3),stride=(1,1),padding=(1,1))\n",
    "        self.fc1=nn.Linear(16*7*7,num_classes)\n",
    "  \n",
    "    def forward(self,x):\n",
    "        x=F.relu(self.conv1(x))\n",
    "        x=self.pool(x)\n",
    "        x=F.relu(self.conv2(x))\n",
    "        x=self.pool(x)\n",
    "        x=x.reshape(x.shape[0],-1)\n",
    "        x=self.fc1(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "model2=CNN()\n",
    "x=torch.randn(64,1,28,28)\n",
    "print(model2(x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# saving checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saving_checkpoint(state,filename=\"my_checkpoint.pth.tar\"):\n",
    "    print(\"saving checkpoint\")\n",
    "    torch.save(state,filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(checkpoint):\n",
    "    print('loading checkpoint')\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size=784\n",
    "num_classes=10\n",
    "learning_rate=0.001\n",
    "batch_size=64\n",
    "num_epochs=5\n",
    "load_model=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning and fine tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity,self).__init__()\n",
    "\n",
    "    def forward(self,x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): Identity()\n",
       "  (classifier): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision\n",
    "#load pretrained model and modify it \n",
    "model=torchvision.models.vgg16(pretrained=True)\n",
    "model.avgpool=Identity()\n",
    "model.classifier=nn.Linear(512,10)\n",
    "model.to(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=datasets.MNIST(root='dataset/',train=True,transform=transforms.ToTensor(), download=True)\n",
    "train_loader=DataLoader(dataset=train_dataset,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "test_dataset=datasets.MNIST(root='dataset/',train=False,transform=transforms.ToTensor(), download=True)\n",
    "test_loader=DataLoader(dataset=test_dataset,batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DataLoader is a PyTorch utility that wraps a dataset and provides an iterable over the dataset with automatic batching, shuffling, and parallel data loading.\n",
    "\n",
    "python\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "dataset: The dataset object to load data from.\n",
    "\n",
    "batch_size: Number of samples per batch.\n",
    "\n",
    "shuffle: Whether to shuffle the data at every epoch (helps to avoid overfitting by providing different mini-batches on each epoch).\n",
    "\n",
    "The DataLoader splits the dataset into manageable batches and shuffles it if specified. It provides an efficient way to iterate over data during training and evaluation.\n",
    "\n",
    "\n",
    "Why Use DataLoader for Accuracy?\n",
    "\n",
    "When calculating accuracy, you need to evaluate the model on batches of data to handle large datasets that cannot fit into memory all at once.\n",
    " The DataLoader helps by:\n",
    "\n",
    "Batching: Automatically splitting the dataset into smaller batches. This is memory efficient and necessary for both training and evaluation, especially for large datasets.\n",
    "\n",
    "Shuffling: Ensuring that the data is shuffled during training to improve generalization. However, shuffling is not strictly necessary during evaluation.\n",
    "\n",
    "Parallel Data Loading: Utilizing multiple workers to load data in parallel, which speeds up the data loading process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=NN(input_size=input_size,num_classes=num_classes).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion=nn.CrossEntropyLoss()\n",
    "optimizer=optim.Adam(model.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading checkpoint\n"
     ]
    }
   ],
   "source": [
    "if load_model:\n",
    "    load_checkpoint(torch.load('my_checkpoint.pth.tar'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving checkpoint\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    if epoch==2:\n",
    "        checkpoint={'state_dict':model.state_dict(),'optimizer':optimizer.state_dict()}\n",
    "        saving_checkpoint(checkpoint)\n",
    "    for batch_index,(data,targets) in enumerate(train_loader):\n",
    "        #get data to device if possible \n",
    "        \n",
    "        data=data.to(device=device)\n",
    "        targets=targets.to(device=device)\n",
    "\n",
    "        #print(data.shape) # here output is 64,1,28,28 means 64 examples , as mnist images are black and white there is only 1 channel so 1 , height =28,width=28 \n",
    "        # But we want this to be in 1 single shape means 784 so we want to unroll this matrix in long vector\n",
    "\n",
    "        #correct shape \n",
    "        data=data.reshape(data.shape[0],-1)  # shape of 64, -1 means 1*28*28\n",
    "\n",
    "        #forward\n",
    "        scores=model(data)\n",
    "        loss=criterion(scores,targets)\n",
    "                              \n",
    "        #backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        #gradient descent or adam step()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certainly! The snippet of code provided seems to be from a PyTorch-based implementation, likely in the context of evaluating a machine learning model's performance. Here's a detailed explanation of each line:\n",
    "\n",
    "```python\n",
    "_, predictions = scores.max(1)\n",
    "num_correct += (predictions == y).sum()\n",
    "num_samples += predictions.size(0)\n",
    "```\n",
    "\n",
    "### Line-by-line Breakdown\n",
    "\n",
    "1. **`_, predictions = scores.max(1)`**:\n",
    "   - `scores`: This is a tensor containing the raw output scores from the model for each class, before applying any activation function like softmax. Its shape is likely `[batch_size, num_classes]`.\n",
    "   - `scores.max(1)`: This function computes the maximum value of each row in the `scores` tensor along the dimension specified (dimension 1 here, which represents the class dimension). It returns two values:\n",
    "     - The first value (`_`), which represents the maximum values themselves. This isn't used in your code, hence the underscore (`_`) to ignore it.\n",
    "     - The second value (`predictions`), which contains the indices of the maximum values (i.e., the predicted class labels for each sample in the batch). `predictions` will be a tensor of shape `[batch_size]`, where each element is an integer representing the predicted class for the corresponding sample.\n",
    "\n",
    "2. **`num_correct += (predictions == y).sum()`**:\n",
    "   - `predictions == y`: This performs an element-wise comparison between `predictions` and `y`, where `y` is the ground truth labels tensor. The result is a tensor of the same shape as `predictions`, containing `True` where the prediction matches the ground truth, and `False` otherwise.\n",
    "   - `(predictions == y).sum()`: This converts the boolean tensor to an integer tensor (with `True` as 1 and `False` as 0) and sums the elements. The result is the number of correct predictions in the current batch.\n",
    "   - `num_correct += ...`: This updates the `num_correct` counter by adding the number of correct predictions from the current batch to the running total.\n",
    "\n",
    "3. **`num_samples += predictions.size(0)`**:\n",
    "   - `predictions.size(0)`: This returns the size of the 0th dimension of the `predictions` tensor, which is the batch size (i.e., the number of samples in the current batch).\n",
    "   - `num_samples += ...`: This updates the `num_samples` counter by adding the batch size to the running total, keeping track of the total number of samples processed so far.\n",
    "\n",
    "### Putting It All Together\n",
    "This code snippet is part of a loop that iterates over batches of data to evaluate a model's accuracy. Here's a summary of its purpose:\n",
    "- **`scores.max(1)`** finds the predicted class for each sample in the batch.\n",
    "- **`num_correct += (predictions == y).sum()`** counts how many predictions were correct in the current batch and updates the total count of correct predictions.\n",
    "- **`num_samples += predictions.size(0)`** updates the total count of samples processed so far.\n",
    "\n",
    "By the end of the loop, `num_correct` will hold the total number of correct predictions across all batches, and `num_samples` will hold the total number of samples processed. These can be used to calculate the overall accuracy of the model as:\n",
    "\n",
    "```python\n",
    "accuracy = num_correct / num_samples\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check accuracy on training and test to see how good our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.eval(): Sets the model to evaluation mode. This is important because it disables dropout and batch normalization, which behave differently during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.no_grad(): Context manager that disables gradient calculation. This reduces memory consumption and speeds up computations, as gradients are not needed during evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking accuracy on training data\n",
      "Got 59100 / 60000 with accuracy 98.50 \n",
      "Checking accuracy on testing data\n",
      "Got 9702 / 10000 with accuracy 97.02 \n"
     ]
    }
   ],
   "source": [
    "def accuracy(loader,model):\n",
    "    if loader.dataset.train:\n",
    "        print(\"Checking accuracy on training data\")\n",
    "    else:\n",
    "        print(\"Checking accuracy on testing data\")\n",
    "    num_correct=0\n",
    "    num_samples=0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x,y in loader:\n",
    "            x=x.to(device=device)\n",
    "            y=y.to(device=device)\n",
    "            x=x.reshape(x.shape[0],-1)\n",
    "            scores=model(x)\n",
    "            _,predictions=scores.max(1)\n",
    "            num_correct+=(predictions==y).sum()\n",
    "            num_samples+=predictions.size(0)\n",
    "\n",
    "    print(f'Got {num_correct} / {num_samples} with accuracy {float(num_correct)/float(num_samples)*100:.2f} ')\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "accuracy(train_loader,model)\n",
    "accuracy(test_loader,model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy function:\n",
    "\n",
    "-> Switches the model to evaluation mode.\n",
    "\n",
    "-> Iterates over batches of data from the provided DataLoader.\n",
    "\n",
    "-> Moves the data and labels to the appropriate device.\n",
    "\n",
    "-> Reshapes the data to the required input format.\n",
    "\n",
    "-> Computes the model's predictions.\n",
    "\n",
    "-> Counts the number of correct predictions.\n",
    "\n",
    "-> Computes and prints the overall accuracy.\n",
    "\n",
    "-> Switches the model back to training mode.\n",
    "\n",
    "-> This function is useful for monitoring the performance of your model on both the training and testing datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
